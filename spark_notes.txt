Spark es un sistema de procesamiento de datos en memoria que distribuye eficientemente el trabajo en varias maquinas
Spark context representa la conexion a un cluster Spark

Acciones 
Transformaciones

Particones de datos
Reduccion de costos de comunicacion entre nodos de trabajo si garantizamos que los datos estan en el mismo nodo

Cuando usar un Dataset?
Cuando hay una semantica compleja, abstraccion de alto nivel, y APIs especificas de dominio
Nuestro procesamiento requiere agregacion, promedios, sumas, consultas SQL y acceso columnar de datos semisestructurados
Necesitamos un mayor grado de segurida de tipo en tiempo de compilacion, queremos objetos JVM escritosy aprovechar el 
optiomizador de consultas Catalyst
Queremos la unificacion y simplificacion de APIs a traves de librerias SPARK

RDD
Es una coleccion distribuida inmutable de elementos de tus datos particionados a traves de nodos en un cluster que pueden 
ser operados en paralelo con una API de bajo nivel que ofrece transformaciones y acciones
Utilizar RDDs cuando: 
Queremos una trasnformacion y acciones de bajo nivel y control sobre nuestro conjunto de datos.
Nuetsros datos no estan estructurados tales como media streams.
Queremos manipular nuestros datos con constructors de programacion funcional en lugar de expresiones especificas de dominio.
Cuando no necesitamos los beneficios disponibles de optimizacion y rendimiento que son caracteristicos de conjunto de datos estructurado.

Codificadores 
Cuando se trata de serializar datos, la API del connjunto de datos tiene el concepto de codificadores que se traducen entre representaciones JVM que son objetos JAVA y el formato binario de SPARK.
SPARK tiene codificadores incorporados como el codificador de tipo integer o entero, el codificador tipo long o largo, estos generan bytecodes para interactuar con datos off-heap y proporcionar acceso a atributos individuales que se requieren sin tener que deserializar un objeto completo.

Row
Los objetos ROW representan registros dentro del conjunto de datos y son simplemente matrices de campos de longuitud fija.
Los objetos ROW tienen una serie de funciones getter para obtener el valor de cada campo segun su indice, el metodo get se ejecuta sobre una columna identificada mediante un numero determinado y nos devuelve una instancia ANY; somos responsables de mandar la instancia al tipo correcto.
Para tipos de datos primitivos, el metodo getType() nos retorna ese tipo de valores.

Dataset y Dataframe
Un Dataset es una coleccion de datos que tiene una estructura y que se pueden tranformar en paralelo mediante operaciones funcionales o relacionales.
Cada conjunto de datos tambien tiene una vista organizada el columnas llamada DATAFRAME, que es un conjunto de datos de objetos tipo ROW.
Los DATASET estan basados en dos caracteristicas de las APIs: APIs fuertemente tipadas y no tipadas.
Considera un DATAFRAME como una vista no tipada de un conjunto de datos, que es un conjunto de datos tipo fila, donde fila es un objeto generico no tipado en la maquina virtual de JAVA.
Un conjunto de datos, por el contrario es una coleccion de datos fuertemente tipados en una maquina virtual de JAVA.
DATASET generalmente es un conjunto de datos estructurados, no necesariamente un ROW sino podria ser de un tipo en particular.

Tipos de SPARK SQL Joins 
-inner -outer -left outer -right outer -left semi

Optimizador Catalyst
SPARK SQL utiliza un optimizador llamado Catalyst para optimizar todas las consultas escritas tanto en SPARK SQL como en el DATAFRAME DSL.
Este optimizador hace que las consultas se ejecuten mucho mas rapidoen comparacion de los RDDs.
El cataliazador es una libreria modular que se constuyo como un sistema basado en reglas, cada regla en el framework se centra en la optmizacion especifica.

Dataframe
Un DATAFRAME es una abstraccion de datos o un lenguaje especifico de dominio para trabajar con datos estructurados o semiestructurados.
Los DATAFRAME almacenan datos de una manera mas eficiente comparados con RDDs nativos, aprovechando su esquema.
Utiliza las capacidades inmutables, en memoria, resilientes, distribuidas y paralelas de un RDD, y aplica una estructura llamada esquema a los datos, permitiendo asi que SPARK administre esta estructuray solo pase datos entre nodos de una manera mucho mas eficiente en lugar de la serializacion de objetos en JAVA.
A diferencia de unn RDD, la informacion esta organizada en columnas, con nombres especificos, similar a una tabla en una base de datos relacional.

SPARK SQL
Datos estructurados son todos aquellos datos que presentan un esquema, es decir, exite un conjunto de campos para cada registro.
SAPARK SQL proporciona un conjunto de datos abstracto que simplifica el trabajo con conjunto de datos estructurados, el conjunto de datos es similar a una tabla en una base de datos relacional.

-La transformacion mapValues garantiza que la clave de cada tupla sea la misma
-Es preferible usar mapValues en lugar de la operacion map
-Uniones son muy caras
-Operaciones que pueden ser afectadas por el particionado
-Operaciones como map podrian hacer que el nuevo RDD pierda la informacion original particionada, ya que esas operaciones pueden, en teoria, cambiar la clave de cada elemento del RDD
*RDD clave-valor*